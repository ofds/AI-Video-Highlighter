import logging
import re
import subprocess
import tempfile
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Callable

import whisper

from .api_client import OpenRouterClient
from .config import (
    OPENROUTER_API_KEY, TRANSCRIPT_FILENAME_SUFFIX, SRT_FILENAME_SUFFIX,
    HIGHLIGHTS_FILENAME_SUFFIX, TEMP_AUDIO_FILENAME_SUFFIX,
    HIGHLIGHT_VIDEO_FILENAME_SUFFIX
)
from .utils import format_timestamp, export_transcript_to_srt

class VideoProcessor:
    """Handles the video processing workflow in distinct stages."""
    def __init__(self, video_path: Path, output_dir: Path, whisper_model: str, llm_model: str, progress_callback: Optional[Callable] = None):
        self.video_path = video_path
        self.output_dir = output_dir
        video_stem = self.video_path.stem
        self.transcript_path = self.output_dir / f"{video_stem}{TRANSCRIPT_FILENAME_SUFFIX}"
        self.srt_path = self.output_dir / f"{video_stem}{SRT_FILENAME_SUFFIX}"
        self.highlights_path = self.output_dir / f"{video_stem}{HIGHLIGHTS_FILENAME_SUFFIX}"
        self.temp_audio_path = self.output_dir / f"{video_stem}{TEMP_AUDIO_FILENAME_SUFFIX}"
        self.highlight_video_path = self.output_dir / f"{video_stem}{HIGHLIGHT_VIDEO_FILENAME_SUFFIX}"
        self.whisper_model = whisper_model
        self.llm_model = llm_model
        self.llm_client = OpenRouterClient(OPENROUTER_API_KEY) if OPENROUTER_API_KEY else None
        self.progress_callback = progress_callback

    def generate_highlights_data(self) -> Tuple[Optional[List[Dict[str, str]]], Optional[List[Dict[str, Any]]]]:
        """
        Runs the analysis part of the pipeline.
        Returns a tuple containing:
        - A list of structured highlight data.
        - The raw whisper transcript segments (or None if transcription was skipped).
        """
        self.output_dir.mkdir(parents=True, exist_ok=True)
        total_steps = 4 

        full_transcript_text = None
        segments: Optional[List[Dict[str, Any]]] = None

        if self.transcript_path.is_file():
            logging.info(f"Transcript file found at '{self.transcript_path}'. Skipping transcription.")
            if self.progress_callback:
                self.progress_callback(1 / total_steps, "Skipping audio extraction...")
                self.progress_callback(2 / total_steps, "Skipping transcription...")
                self.progress_callback(3 / total_steps, "Skipping transcript saving...")
            full_transcript_text = self.transcript_path.read_text(encoding="utf-8")
            # Note: 'segments' will be None here, so SRT export will be disabled in the GUI.
        else:
            if not self._extract_audio():
                return None, None
            if self.progress_callback: self.progress_callback(1 / total_steps, "Audio extracted.")
            
            segments = self._transcribe_audio()
            if self.progress_callback: self.progress_callback(2 / total_steps, "Audio transcribed.")
            
            if segments:
                self._save_transcripts(segments)
                if self.progress_callback: self.progress_callback(3 / total_steps, "Transcripts saved.")
                full_transcript_text = self._format_transcript_for_llm(segments)
            else:
                return None, None 
        
        if full_transcript_text:
            highlights_text = None
            if self.highlights_path.is_file():
                logging.info(f"Highlights file found at '{self.highlights_path}'. Skipping LLM generation.")
                if self.progress_callback: self.progress_callback(4 / total_steps, "Parsing existing highlights...")
                highlights_text = self.highlights_path.read_text(encoding="utf-8")
            elif self.llm_client:
                highlights_text = self._generate_and_save_highlights(full_transcript_text, self.llm_model)
                if self.progress_callback: self.progress_callback(4 / total_steps, "Highlights generated by LLM.")
            else:
                logging.warning("OPENROUTER_API_KEY not set. Skipping highlight generation.")
                return None, segments
            
            if highlights_text:
                parsed_highlights = self._parse_highlights_to_structured_data(highlights_text)
                return parsed_highlights, segments

        return None, segments

    def create_highlight_video(self, time_segments: List[Tuple[str, str]]):
        """
        Creates the final highlight video from a list of selected time segments.
        """
        logging.info(f"Creating highlight video from {len(time_segments)} selected segments...")
        if not time_segments:
            logging.warning("No time segments provided to create_highlight_video. Aborting.")
            return

        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            clip_files = []
            for i, (start, end) in enumerate(time_segments):
                clip_filename = temp_path / f"clip_{i}.mp4"
                command = ["ffmpeg", "-y", "-i", str(self.video_path), "-ss", start, "-to", end, "-c", "copy", str(clip_filename)]
                try:
                    subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                    clip_files.append(clip_filename)
                except (subprocess.CalledProcessError, FileNotFoundError) as e:
                    logging.error(f"Failed to create clip {i} with ffmpeg.")
                    if isinstance(e, subprocess.CalledProcessError): logging.error(f"FFmpeg stderr: {e.stderr.decode()}")
                    return
            
            if not clip_files:
                logging.error("No clips were successfully created, cannot generate highlight video.")
                return

            concat_list_path = temp_path / "concat_list.txt"
            with open(concat_list_path, "w", encoding="utf-8") as f:
                for clip in clip_files: f.write(f"file '{clip.resolve()}'\n")

            concat_command = ["ffmpeg", "-y", "-f", "concat", "-safe", "0", "-i", str(concat_list_path), "-c", "copy", str(self.highlight_video_path)]
            try:
                subprocess.run(concat_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                logging.info(f"âœ… Successfully created highlight video: {self.highlight_video_path}")
            except (subprocess.CalledProcessError, FileNotFoundError) as e:
                logging.error(f"Failed to stitch clips with ffmpeg.")
                if isinstance(e, subprocess.CalledProcessError): logging.error(f"FFmpeg stderr: {e.stderr.decode()}")
        
        self._cleanup()

    def _extract_audio(self) -> bool:
        logging.info(f"Extracting audio from '{self.video_path.name}'...")
        command = ["ffmpeg", "-y", "-i", str(self.video_path), "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", str(self.temp_audio_path)]
        try:
            subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)
            logging.info(f"Audio extracted successfully to '{self.temp_audio_path}'.")
            return True
        except (FileNotFoundError, subprocess.CalledProcessError) as e:
            logging.error(f"Error during audio extraction. Make sure FFmpeg is installed and in your PATH. Details: {e}")
            return False

    def _format_transcript_for_llm(self, segments: List[Dict[str, Any]]) -> str:
        """Formats the transcript segments into a single string for the LLM."""
        return "".join(
            f"[{format_timestamp(s['start'])}] {s['text'].strip()}\n" for s in segments
        )

    def _transcribe_audio(self) -> Optional[List[Dict[str, Any]]]:
        logging.info(f"Loading Whisper model '{self.whisper_model}'...")
        try:
            model = whisper.load_model(self.whisper_model)
            logging.info("Model loaded. Starting transcription...")
            result = model.transcribe(str(self.temp_audio_path), fp16=False)
            logging.info("Transcription complete.")
            return result.get("segments")
        except Exception as e:
            logging.error(f"Error during transcription: {e}")
            return None

    def _generate_and_save_highlights(self, full_transcript: str, llm_model: str) -> Optional[str]:
        highlights = self.llm_client.get_highlights_from_transcript(full_transcript, llm_model)
        if highlights:
            with open(self.highlights_path, "w", encoding="utf-8") as f: f.write(highlights)
            logging.info(f"Highlights saved to {self.highlights_path}")
            return highlights
        logging.error("Could not retrieve highlights from the LLM.")
        return None

    def _parse_highlights_to_structured_data(self, highlights_text: str) -> List[Dict[str, str]]:
        logging.info("Parsing structured highlights from text...")
        highlights = []
        try:
            moments_block_match = re.search(r"Interesting_Moments:\s*```(.*?)```", highlights_text, re.DOTALL)
            if not moments_block_match:
                logging.warning("Could not find 'Interesting_Moments' block in highlights text.")
                return []

            moments_text = moments_block_match.group(1).strip()
            moment_entries = re.split(r'\n?\d+\.\s*', moments_text)
            
            for entry in moment_entries:
                entry = entry.strip()
                if not entry:
                    continue

                title_match = re.search(r"Title:\s*(.*)", entry)
                start_time_match = re.search(r"Start_Time:\s*(\d{2}:\d{2}:\d{2})", entry)
                end_time_match = re.search(r"End_Time:\s*(\d{2}:\d{2}:\d{2})", entry)
                why_match = re.search(r"Why_Interesting:\s*(.*)", entry, re.DOTALL)

                if all([title_match, start_time_match, end_time_match, why_match]):
                    highlights.append({
                        "title": title_match.group(1).strip(),
                        "start_time": start_time_match.group(1).strip(),
                        "end_time": end_time_match.group(1).strip(),
                        "why": why_match.group(1).strip().replace('\n', ' ')
                    })
                else:
                    logging.warning(f"Could not fully parse a highlight entry: '{entry}'")

        except Exception as e:
            logging.error(f"An unexpected error occurred while parsing highlights: {e}")
            return []
            
        logging.info(f"Successfully parsed {len(highlights)} highlights.")
        return highlights

    def _save_transcripts(self, segments: List[Dict[str, Any]]):
        with open(self.transcript_path, "w", encoding="utf-8") as f:
            f.write(self._format_transcript_for_llm(segments))
        logging.info(f"Transcript saved to {self.transcript_path}")
        
        # Save SRT using the new utility function
        export_transcript_to_srt(segments, self.srt_path)

    def _cleanup(self):
        try:
            if self.temp_audio_path and self.temp_audio_path.exists():
                self.temp_audio_path.unlink()
                logging.info(f"Removed temporary audio file: {self.temp_audio_path}")
        except OSError as e:
            logging.error(f"Error removing temporary audio file: {e}")